{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9QLe_T6GZUd"
   },
   "source": [
    "# Задание на программирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYlIf2yHv8hz"
   },
   "source": [
    "**Выполнять задание следует с текущими значениями гиперпараметров. Для проверки ниже будут приведены ответы, которые должны получиться в результате выполнения задания. После того, как все ответы совпадут, можно будет использовать полученный блокнот для выполнения индивидуального задания.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDQzNIZXAoFE"
   },
   "source": [
    "Зададим гиперпараметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOMw2ZbOAmOZ"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1 # Параметр эпсилон при использовании эпсилон жадной стратегии\n",
    "gamma = 0.8 # Коэффциент дисконтирования гамма\n",
    "random_seed = 100 #Random seed\n",
    "time_delay = 1 # Задержка времени при отрисовке процесса игры после обучения (секунды)\n",
    "lr_rate = 0.9 #Коэффициент скорости обучения альфа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQu5IYHX8jId"
   },
   "source": [
    "Импортируем библиотеки, создаем свою среду размера 6х6. S обозначает точку старта. F -- лед безопасен, H -- проталина, G -- цель. Параметр `is_slippery=False` отвечает за условное отсутствие скольжения. То есть если агент выбрал действие пойти направо, то он переместится в соответствующее состояние. В общем случае из-за \"скольжения\" можно оказаться в другом состоянии. Мы также скопировали из библиотки GYM и слегка модифицировали функцию ```generate_random_map ```, для того, чтобы генерировать произвольные карты на основе ```random_seed ```.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym==0.12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install pygame\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1541,
     "status": "ok",
     "timestamp": 1591260881593,
     "user": {
      "displayName": "Дмитрий Волчек",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN8jjHr8b6AlqmkI_BBSJH_qXpcmh3mgxKpBXd=s64",
      "userId": "12396512946067001179"
     },
     "user_tz": -180
    },
    "id": "awL7CCCwD6C3",
    "outputId": "5b2d42db-dc19-4cef-f753-805b8b6be9c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваша карта\n",
      "\n",
      "\u001b[41mS\u001b[0mFFHFF\n",
      "FHFFHF\n",
      "FFFHHF\n",
      "HFFHHF\n",
      "FFFFFF\n",
      "FFFFFG\n",
      "<TimeLimit<FrozenLakeEnv<FrozenLake-v1>>>\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def generate_random_map(size, p, sd):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    np.random.seed(sd)\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0,0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r,c) in discovered:\n",
    "                discovered.add((r,c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == 'G':\n",
    "                        return True\n",
    "                    if (res[r_new][c_new] not in '#H'):\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
    "        res[0][0] = 'S'\n",
    "        res[-1][-1] = 'G'\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "\n",
    "#Генерация карты\n",
    "random_map = generate_random_map(size=6, p=0.8, sd = random_seed) #Создаем свою карту\n",
    "env = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=False) #Инициализируем среду\n",
    "print(\"Ваша карта\")\n",
    "env.render() #Выводим карту на экран\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDCexoEU9a_c"
   },
   "source": [
    "Функции выбора действия и обновления таблицы ценности действий. Строчка *** используется для того, чтобы проверять ответы в openedx. Вне рамках академической задачи лучше использовать оригинальный метод класса `environment`, то есть:\n",
    "\n",
    "`action = env.action_space.sample()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5TbDqn6G_Pt"
   },
   "source": [
    "# Задача 1\n",
    "Дополните функцию ```learn()```, чтобы в результате ее вызова обновлялось значение ценности текущего действия согласно алгоритму Q-обучения\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdQBpxaTOK7u"
   },
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0,env.action_space.n) #***\n",
    "    else:\n",
    "        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
    "    return action\n",
    "\n",
    "def learn(state, state2, reward, action, done):\n",
    "    # мой код ниже\n",
    "    if state2 == done:\n",
    "        Q[state, action] =  Q[state, action] + lr_rate*(reward - Q[state, action])\n",
    "    else:\n",
    "        Q[state, action] =  Q[state, action] + lr_rate*(reward + gamma*np.amax(Q[state2, :]) - Q[state, action])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7COGeyA_Ist3"
   },
   "source": [
    "# Задача 2\n",
    "Дополните следующий код так, чтобы в результате обучения модели можно было узнать количество побед и номер игры (`game`), на котором агент впервые одержал пятую победу подряд."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0adDl7NvJoQP"
   },
   "source": [
    "Поясним, что возвращает функция ```env.step(action)```\n",
    "\n",
    "```state2``` -- следующее состояние\n",
    "\n",
    "```reward``` -- награда\n",
    "\n",
    "```done``` -- флаг окончания игры. True в случае победы или падения в проталину. False в остальных случаях.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38517,
     "status": "ok",
     "timestamp": 1591261203688,
     "user": {
      "displayName": "Дмитрий Волчек",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN8jjHr8b6AlqmkI_BBSJH_qXpcmh3mgxKpBXd=s64",
      "userId": "12396512946067001179"
     },
     "user_tz": -180
    },
    "id": "aq92-dWiOchF",
    "outputId": "91ec4dc4-fb39-4818-ac78-79c9fe6d0ee7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2884.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Inititalization\n",
    "np.random.seed(random_seed)\n",
    "total_games = 10000\n",
    "max_steps = 100\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#Main cycle\n",
    "wins = 0\n",
    "five = 0\n",
    "counter = 0\n",
    "for game in tqdm(range(total_games)):\n",
    "    state = env.reset()\n",
    "    t = 0\n",
    "    while t < max_steps:\n",
    "        \n",
    "        t += 1\n",
    "\n",
    "        action = choose_action(state)\n",
    "\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        \n",
    "        #\n",
    "        if reward > 0.:\n",
    "            counter += 1\n",
    "        #\n",
    "        #print('state2 = ', state2, 'reward = ',reward, 'action = ', action, ', info = ', info.items())\n",
    "        if t == max_steps:\n",
    "            done = True  \n",
    "\n",
    "        learn(state, state2, reward, action, done)\n",
    "\n",
    "        state = state2\n",
    "        #print('state = ', state)\n",
    "        if done:\n",
    "            #print(reward)\n",
    "            if reward:\n",
    "                wins += 1\n",
    "                five += 1\n",
    "                if five == 5:\n",
    "                    number = game                  \n",
    "            else:\n",
    "                if five < 5:\n",
    "                    five = 0\n",
    "            \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFuxsqdRLOS9"
   },
   "source": [
    "Вывод ответов при заданных параметрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZbJtFnhLa7w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7851\n",
      "Количество побед в серии из 10 000 игр:  7851\n",
      "Пять побед подряд впервые было одержано в игре  885\n"
     ]
    }
   ],
   "source": [
    "print(counter)\n",
    "print(\"Количество побед в серии из 10 000 игр: \", wins)\n",
    "print(\"Пять побед подряд впервые было одержано в игре \", (number+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSXdSiG2WI71"
   },
   "source": [
    "Должны получиться следующие результаты.\n",
    "\n",
    "\n",
    "*  Количество побед в серии из 10 000 игр:  7914\n",
    "*  Пять побед подряд впервые было одержано в игре  885\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nazZaAbwQGBt"
   },
   "source": [
    "Произведем одну игру, чтобы проследить за действиями агента. При этом будем считать модель полностью обученной, то есть действия выбираются жадно, значения ценностей действий в таблице не обновляются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10924,
     "status": "ok",
     "timestamp": 1591261275269,
     "user": {
      "displayName": "Дмитрий Волчек",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN8jjHr8b6AlqmkI_BBSJH_qXpcmh3mgxKpBXd=s64",
      "userId": "12396512946067001179"
     },
     "user_tz": -180
    },
    "id": "5ysllZjEQXLa",
    "outputId": "29ec2e79-a0d5-4fcb-a551-6209d40dd7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!Победа!!!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Жадный выбор действий\n",
    "def choose_action_one_game(state):\n",
    "    action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
    "    return action\n",
    "\n",
    "states=[]#Массив для сохранения состояний агента в течение игры\n",
    "t = 0\n",
    "state = env.reset()\n",
    "wn = 0\n",
    "while(t<100):\n",
    "  env.render()\n",
    "  time.sleep(time_delay)\n",
    "  clear_output(wait=True)\n",
    "  action = choose_action_one_game(state)  \n",
    "  state2, reward, done, info = env.step(action)  \n",
    "  states.append(state)\n",
    "  state = state2\n",
    "  t += 1\n",
    "  if done and reward == 1:\n",
    "    wn=1\n",
    "  if done:\n",
    "    break\n",
    "if wn == 1:\n",
    "  print(\"!!!Победа!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x696NulpReFI"
   },
   "source": [
    "Отобразим маршрут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1071,
     "status": "ok",
     "timestamp": 1591261279569,
     "user": {
      "displayName": "Дмитрий Волчек",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgN8jjHr8b6AlqmkI_BBSJH_qXpcmh3mgxKpBXd=s64",
      "userId": "12396512946067001179"
     },
     "user_tz": -180
    },
    "id": "UKMCMdpOTcXy",
    "outputId": "bd9a32aa-b615-407f-bb4b-9a2ae654df4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d8832b4df0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAP3UlEQVR4nO3db2wc9Z3H8c/kD0ZrhzgCslFj7IWqMa1yNHebtuEKsq17cAURXf886DlbjoaU7R+dTm4K9IrFBSq56ul8Oj8ACdmq4AFbW+nRNndBVY87vC5F2qrx1QKiS7jeBRs3iQs5DLa3MfnzuweDMRuvHW8yX8/O5P2SRvbMTma+bDfvzk7WieecEwBYWBX2AADii8AAMENgAJghMADMEBgAZtZUsvN1113nUqmU0SjBm5mZUW1tbdhjLEuUZpWk48eP68SJE2GPsWw333xzpJ7fqL0ehoeH33TOXb/gAefcspd0Ou2iZHBwMOwRli1KszrnXHd3t5MUmSVqz2/U5pV0yJVpRkVXMHM2dW/SxMzEpfzSFZGsTerk/SfDHgO44l3SPZhqjotU/fMBVwpu8gIwQ2AAmCEwAMwQGABmCAwAMwQGgBkCA8AMgQFghsAAMENgAJghMADMEBgAZggMADMEBoAZAgPATLiBeald+qdj0iPn/K8vtYc6DoBgXdLfaBeIl9qlf+2Tzrz3946+nfLXJemW/tDGAhCc8K5g/uN783GZc6bW3w4gFsILzNuNlW0HEDnhBWb9WGXbAUROeIH5s4ektTOl29bO+NsBxEJ4gbmlX9p5n7T6tCQnrX/NX+cGLxAbgQfm0zd8Wi/e+6Imvz2pUw+e0i93/1LbP7Rd93z8Hr2w+4XSnW/plxoKUtOQ9M0bLxqXpvVNcvucVnurgx4bgIFA/5h63VXrdHDXQX392a9r/+H9umr1Vbq98XbNnp297GMTFSB6Ar2C2XLtFknSwCsDOu/O6/TZ03ruf5/TmfNn9MRdT+jWhls19Z0pvfXttyRJd37kTv3ns2m9/fJtGusY076Wfe8fa+5q5d4/vlejHaN6/p7n9Yvdv5AkTf7tpKa+M6UdDTuCHB9AwAK9gnn11Ks6d/6cnvqLpzRweECF8YImT0/qyJtH9LWDX9NX/uQruv3J29/ff+bdGf3V3iM6/OqMtj74N3ru7uc0cnJEB44eeH+flqYWffTxj+q8O69kbVKvdbym+u/X65w7F+ToAAwEegUz9e6UbnvyNjk59e3s0xsPvKEDf3lAG2s3lt1/aHRIrxydkXPSy79/Wf2v9Ksl1VKyzyP5R1Q8U9Tps6eDHBXACgj8RwWOvHlEuw/sliQ1X9uspz//tHr+vEc//5+fL9j3k5s/qe/3f1xbt9TqqtpJ1ayp0Y8O/6hkn9ffeT3oEQGsENM/pj566qieGnlKWzdulZNb8PgPP/9D/cu/n9INf1pQ/d/X64lDT8jzvJJ9nJv/deWOAaB6BRqY5mubtffWvdq8brMkqeGaBrVvbVfhdwVNTE+o4ZoGrV219v3919Ws0/9NntHs7Hl94kOf0K4/2rXk8d+YeUPnzp/TTRtuCnJsAEYCfYs09e6UPrX5U9q7Y6/qr67X5OlJHfzvg3rg3x7Q6bOndfj3h3Xy/pM6787r+n+4Xt949hv6x28+rcce/YiGjv+d9h/er/qr6xc9/h/O/kFdL3TpxXtf1NrVa/WZpz+jX/3uV0H+JwAIUKCBOT51XF/85y8u+vhd/XeVrD/zX8/omQf/2l/ZvbPksdG3R+U9Wvp2SZL25fdpX37fgu0Aqg9/ox0AMwQGgBkCA8AMgQFghsAAMENgAJi5pMAka5NBzxGoap8PuFJc0udgTt5/MrABWgf9r/l9/BgAEDfeB3/Wp+wOnpeVlJWkZDKZHhgYCHSAjo5tkqSenpFAjytJExMTGh8fD/y4FhoaGpRMRufKa3p6WnV1dWGPsWxRei1IUnNzc6Se37a2tmHn3PYFDzjnlr2k02kXtJYWf7HQ3d3tJEVi6e7utnkSjAwODoY9QkWi9FqQFLnnV9IhV6YZ3OQFYIbAADBDYACYITAAzBAYAGYIDAAzBAaAGQIDwAyBAWCGwAAwQ2AAmCEwAMwQGABmCAwAMwQGgBkCA8AMgQFgJtTA5HJSoSANDUmplL8OID5CC0wuJ2Wz0uysvz466q8TGSA+QgtMZ6dULJZuKxb97QDiIbTAjI1Vth1A9IQWmMbGyrYDiJ7QAtPVJSUSpdsSCX87gHgILTCZjNTbK9XU+OtNTf56JhPWRACCdkn/dGxQMhmpr8//Pp8PcxIAFvigHQAzBAaAGQIDwAyBAWCGwAAwQ2AAmCEwAMwQGABmCAwAMwQGgBkCA8AMgQFghsAAMENgAJghMADMEBgAZggMADOec27pHTwvKykrSclkMj0wMBDoAB0d2yRJPT0jgR5Xkqanp1VXVxf4cS1EaVZJmpiY0Pj4eNhjLFtDQ0Ok5m1ubo7U66GtrW3YObd9wQPOuWUv6XTaBa2lxV8sDA4O2hzYQJRmdc657u5uJykyS9TmjdrrQdIhV6YZvEUCYIbAADBDYACYITAAzBAYAGYIDAAzBAaAGQIDwAyBAWCGwAAwQ2AAmCEwAMwQGABmCAwAMwQGgBkCA8AMgQFgJtTA5HJSoSANDUmplL9erXI5f8ZVq6p/Vil68yKe1oR14lxOymal2Vl/fXTUX5ekTCasqcqbm7VY9NereVYpevMivkILTGfn/G+AOcWitGeP1NcXzDkmJ7epvv7yj1MozIdwTrXOKi0+b2cngcHKCu0t0thY+e0X/saoBovNVI2zSovPtdhzDlgJ7QqmsdG/dL9QU5OUzwdzjnx+RK2trZd9nFQqOrNKi8/b2BjI4YFlC+0KpqtLSiRKtyUS/vZqE6VZpejNi/gKLTCZjNTb618FeJ7/tbe3Ou8RRGlWaX7emhp/vdrnRXyF9hZJ8l/wUXnRR2lWyZ917gZ0UG/jgErxQTsAZggMADMEBoAZAgPADIEBYIbAADBDYACYITAAzBAYAGYIDAAzBAaAGQIDwAyBAWCGwAAwQ2AAmCEwAMxUFJjh4WF5nheZBUC4POfc0jt4XlZSVpLWr1+ffvjhh1dirkA0Nzerrq4u7DGWZXp6OvBZOzq2SZJ6ekYCPa4kTUxMaHx8PPDjWmloaIjUvFF67UpSW1vbsHNu+4IHnHPLXiS5KC2Dg4MuKixmbWnxFwvd3d2h/+9byRK1eaP02nXOOUmHXJlmcA8GgBkCA8AMgQFghsAAMENgAJghMADMEBgAZggMADMEBoAZAgPADIEBYIbAADBDYACYITAAzBAYAGYIDAAzBAaAGQITU7mcVChIQ0NSKuWvAyuNwMRQLidls9LsrL8+OuqvExmsNAITQ52dUrFYuq1Y9LcDK4nAxNDYWGXbASsEJoYaGyvbDlghMDHU1SUlEqXbEgl/O7CSCEwMZTJSb69UU+OvNzX565lMuHPhyrMm7AFgI5OR+vr87/P5UEfBFYwrGABmCAwAMwQGgBkCA8AMgQFghsAAMENgAJghMADMEBgAZggMADMEBoAZAgPADIEBYIbAADBDYACYITAAzFQUmHQ6LedcZBbYidprIWrzxoV3sf8Yz/OykrKSlEwm0wMDAysxVyCmp6dVV1cX9hjLYjFrR8c2SVJPz0igx5Wi9dxKzGutra1t2Dm3fcEDFf6/gIuSwcHBsEdYNotZW1r8xUKUnlvnmNeapEOuTDO4BwPADIEBYIbAADBDYACYITAAzBAYAGYIDAAzBAaAGQIDwAyBAWCGwAAwQ2AAmCEwAMwQGABmCAwAMwQGgBkCA8AMgYmpXE4qFKShISmV8terWS7nz7lqFfPGyZqwB0Dwcjkpm5VmZ/310VF/XZIymfDmWszcvMWiv8688UFgYqizc/7FP6dYlPbskfr6gjnH5OQ21dcHc6xCYT6Gc6I4b2cngbkQb5FiaGys/PYLf1NUi8Xmitq8iz3vVzKuYGKosdG/bL9QU5OUzwdzjnx+RK2trYEcK5WKx7yNjYEcPla4gomhri4pkSjdlkj426sR88YXgYmhTEbq7fWvADzP/9rbW733B6I6b02Nv17t84aJt0gxlclE6wUfxXnnbkAH9TYujriCAWCGwAAwQ2AAmCEwAMwQGABmCAwAMwQGgBkCA8AMgQFghsAAMENgAJghMADMEBgAZggMADMEBoAZAgPAjOecW3oHz8tKykpSMplMDwwMrMRcgZienlZdXV3YYyxLlGaVmFeSOjq2SZJ6ekYCPa4Uvee3ra1t2Dm3fcEDzrllL+l02kXJ4OBg2CMsW5RmdY55nXOupcVfLETt+ZV0yJVpBm+RgGq2aZP/FxVX67Jp05LjExigmk1MhD3B0i4yH4EBYIbAADBDYACYITAAzBAYAGYIDAAzBAaAGQIDwAyBAWCGwAAwQ2AAmCEwAMwQGABmCAwAMwQGgBkCA1yCXE4qFKShISmV8terWU7tSumYVumcUjqmnNpX5LxrVuQsQIzkclI2K83O+uujo/66JGUy4c21mJzalVWfiqqVJI0qpaz6JEkZ9Zuem8AAFerslIrF0m3ForRnj9TXF8w5Jie3qb5ekgYv+1gF7dCsri7ZVlStOvU988DwFgmo0NhY+e1zVzTVZlY1ZbePqdH83FzBABVqbPTfFl2oqUnK54M5Rz4/otbWVslru+xjpXRMo0ot2N6oRUoZIK5ggAp1dUmJROm2RMLfXo269JASminZltCMuvSQ+bkJDFChTEbq7fWvWDzP/9rbG+IN3qkp6cYbF304o3716j416TVNTTndduO4enXf4vdfWlqk118PZDTeIgGXIJMJKSjHjknJpHTu3Py2LVukEyeW/GUZ9ftBWSe9YDziB3EFA0TNzp3SunXzy0XiEiYCA0Sdc9KHP+x//+ST0mOPSQcPSu+8438a8Kabyu97xx3S4cP+fuPj0re+VXrcvXv9f1jt+HHpy1++pNEIDBA37e3So49KGzZIv/3t4neff/AD6atfla65Rtq6VXr++fnHNm2S1q+XNm/2P+Dz+ON674M5FSEwQNT89KfSW2/5y09+svDxH/9Y+vWv/fs0uZy0bVv545w5I33sY/7brMlJ6Te/KX3su9+Vzp6VfvYzaXpaam6ueFQCA0TNZz/rX51s2CB97nMLHz95cv77YlGqqyt/nC98QbrzTv9DPfm8tGPH/GOnTpXeSF7qOEsgMMCV6tAhP1YbN/pXRfv3B34KAgNcidaulXbt8u+/nD3r3+j94BVLQPgcDHCluvtu/0+cVq+Wjh6VvvSlwE9BYIAoKfeJXc+b/3737tLHhoakG24ov+8dd5Q/x4W/ZrHzLgNvkQCYITAAzBAYAGYIDAAzBAaAGQIDwAyBAapZMhn2BEu7yHx8DgaoZh/8uaIIumhgPM/LSnrvX33RtOd5R21HCtR1kt4Me4hlitKsEvNai9q8ZX/U2nPOrfQgK8bzvEPOue1hz7EcUZpVYl5rcZmXezAAzBAYAGbiHpjesAeoQJRmlZjXWizmjfU9GADhivsVDIAQERgAZggMADMEBoAZAgPAzP8DPXX625RmejIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_maze_pic(maze):\n",
    "  maze_pic=[]\n",
    "  for i in range(len(maze)):\n",
    "    row = []\n",
    "    for j in range(len(maze[i])):\n",
    "      if maze[i][j] == 'S':\n",
    "        row.append(0)\n",
    "      if maze[i][j] == 'F':\n",
    "        row.append(0)\n",
    "      if maze[i][j] == 'H':\n",
    "        row.append(1)\n",
    "      if maze[i][j] == 'G':\n",
    "        row.append(0)\n",
    "    maze_pic.append(row)\n",
    "  maze_pic = np.array(maze_pic)\n",
    "  return maze_pic\n",
    "  \n",
    "\n",
    "#Make maze fit to plot\n",
    "maze_pic = make_maze_pic(random_map)\n",
    "nrows, ncols = maze_pic.shape\n",
    "\n",
    "#Arrays of picture elements\n",
    "rw = np.remainder(states,nrows)\n",
    "cl = np.floor_divide(states,nrows)\n",
    "if wn == 1:\n",
    "  rw = np.append(rw, [nrows-1])\n",
    "  cl = np.append(cl,[ncols-1])\n",
    "\n",
    "#Picture plotting\n",
    "fig, ax1 = plt.subplots(1, 1, tight_layout=True)\n",
    "ax1.clear()\n",
    "ax1.set_xticks(np.arange(0.5, nrows, step=1))\n",
    "ax1.set_xticklabels([])\n",
    "ax1.set_yticks(np.arange(0.5, ncols, step=1))\n",
    "ax1.set_yticklabels([])\n",
    "ax1.grid(True)\n",
    "ax1.plot([0],[0], \"gs\", markersize=40)  # start is a big green square\n",
    "ax1.text(0, 0.2,\"Start\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Start text\n",
    "ax1.plot([nrows-1],[ncols-1], \"rs\", markersize=40)  # exit is a big red square\n",
    "ax1.text(nrows-1, ncols-1+0.2,\"Finish\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Exit text\n",
    "ax1.plot(rw,cl, ls = '-', color = 'blue') #Blue lines path\n",
    "ax1.plot(rw,cl, \"bo\")  # Blue dots visited cells\n",
    "ax1.imshow(maze_pic, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5m14YFyrI6M0"
   },
   "source": [
    "# Задача 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lb3BvDuBxTO0"
   },
   "source": [
    "Дублируйте полученный блокнот и используйте вместо алгоритма Q-обучения алгоритм SARSA. Обратите внимание на то, что в задании требуется изменить количество игр. То есть `total_games = 40000`. Запускать блоки следует последвательно с самого начала (из-за `random_seed`). Отдельно обращаем ваше внимание на то, что при изменении алгоритма с Q-обучения на SARSA модификации подлежит как процесс обучения, так и функция `learn()`. Кроме того, у функции `learn()` должен появиться дополнительный аргумент (следующее действие). Ниже приведен фрагмент кода с пояснениями, как именно нужно модифицировать алгоритм.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSVpTwTAJO7d"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "np.random.seed(random_seed)\n",
    "total_games = 40000\n",
    "max_steps = 100\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#Main cycle\n",
    "for game in tqdm(range(total_games)):\n",
    "    state = env.reset()\n",
    "    t = 0\n",
    "    action = #Выбор действия в самом начале каждой игры\n",
    "    while t < max_steps:\n",
    "              \n",
    "        t += 1\n",
    "\n",
    "        state2, reward, done, info = env.step(action)\n",
    "\n",
    "        action2 =  #выбор действия как для следующего шага игры, так и для обновления ценности совершенного действия\n",
    "\n",
    "        if t == max_steps:\n",
    "          done = True  \n",
    "\n",
    "        learn(state, state2, reward, action, action2, done) # action2 также передается в функцию обучения\n",
    "\n",
    "        state = state2\n",
    "\n",
    "        action = action2\n",
    "\n",
    "        if done:\n",
    "          break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RB_PX2vYIY0-"
   },
   "source": [
    ". В результате обучения должны получиться следующие ответы:\n",
    "\n",
    "\n",
    "\n",
    "*   Количество побед в серии из 40 000 игр:  32328\n",
    "*   Пять побед подряд впервые было одержано в игре  894"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO9VDoqg7hUVb1kflsc9dSN",
   "collapsed_sections": [],
   "name": "Programming_students_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
